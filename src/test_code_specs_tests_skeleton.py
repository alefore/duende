# DO NOT EDIT. This file is automatically generated by Duende.
# DM validator:
# MYPYPATH=~/coding-agent/src mypy $DMPATH && ~/local/bin/python3 $DMPATH
#
# These tests have a few constraints:
#
# * Mock is NOT allowed in this code. These tests should let all the
#   dependencies  of CodeSpecsTestsSkeleton be used directly.
#
# * These tests are not allowed to implement BaseAgentLoop directly; instead,
#   they must always use AgentLoop and AgentLoopFactory.
#
# * These tests must NOT subclass AgentLoop nor AgentLoopFactory, nor mock any
#   of their methods.
#
# * These tests must NOT contain the ðŸ„ character anywhere (in order to avoid
#   possible clashes with Duende, since that is already interpreting markers).
#
# * If they need to get a conversation, these tests must get it from the
#   self.conversation_factory. They must NOT get it from the conversational_ai.

import aiofiles
from collections import defaultdict
import os
import pathlib
import re
import shutil
import tempfile
from typing import Callable, Awaitable, Any
import unittest

from agent_command import Argument, AgentCommand, CommandOutput, CommandInput, AgentCommand, ArgumentContentType, VariableMap, VariableName, VariableValue, VariableValueStr
from agent_loop import AgentLoop, AgentLoopFactory
from agent_loop_options import AgentLoopOptions
from agent_workflow_options import AgentWorkflowOptions
from code_specs import PathAndValidator, Validator, prepare_command_registry, prepare_initial_message, run_agent_loop, ValidationResult, MarkerChar, MarkersOverlapError, MarkerName
from code_specs_tests_skeleton import CodeSpecsTestsSkeletonWorkflow, tests_skeleton_variable, MUSHROOM, HEDGEHOG, path_to_test_variable
from command_registry import CommandRegistry
from confirmation import ConfirmationManager, ConfirmationState
from conversation import Conversation, ConversationId, ConversationFactory, ConversationFactoryOptions
from conversation_state import ConversationState
from conversational_ai import ConversationalAI
from conversational_ai_test_utils import FakeConversationalAIConversation, FakeConversationalAI
from done_command import DoneCommand
from file_access_policy import FileAccessPolicy
from message import ContentSection, Message
from selection_manager import SelectionManager


class TestFileAccessPolicy(FileAccessPolicy):

  def allow_access(self, path: str) -> bool:
    return True  # Allow all access for testing


class TestConfirmationState(ConfirmationState):

  async def RequireConfirmation(self, conversation_id: int,
                                prompt: str) -> str | None:
    return None  # Always confirm


class TestConfirmationManager(ConfirmationManager):

  async def RequireConfirmation(self, conversation_id: ConversationId,
                                message: str) -> str | None:
    return None  # Always confirm for tests


class TestSelectionManager(SelectionManager):
  pass  # Default implementation is fine for now


class TestCodeSpecsTestsSkeletonWorkflow(unittest.IsolatedAsyncioTestCase):

  def setUp(self):
    self.conversation_factory = ConversationFactory(
        ConversationFactoryOptions())

  async def done_message_for_file(self, contents: str) -> Message:
    """Returns a Message calling `done` with `path_to_test_variable`.

    The contents are written to a temporary file; its path is given to
    `path_to_test_variable`. Clean-up of the temporary file is scheduled.

    The returned messages can be given directly to a FakeConversationalAI's list
    of messages. That implies that role should be "assistant".
    """
    # âœ¨ done message for contents
    f = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.py')
    f.write(contents)
    f.close()
    tmp_path = pathlib.Path(f.name)
    self.addCleanup(os.remove, tmp_path)

    return Message(
        role="assistant",  # Changed from "user" to "assistant" as per new docstring.
        content_sections=[
            ContentSection(
                content=f"Calling done command with {path_to_test_variable} = {tmp_path}",
                command=CommandInput(
                    command_name="done",
                    args=VariableMap({
                        path_to_test_variable: VariableValueStr(str(tmp_path))
                    })),
                summary=f"Done command for path: {tmp_path}")
        ])
    # âœ¨

  def get_valid_contents_with_hedgehog_markers(self) -> str:
    """Returns a file (contents) with valid `"{{"+HEDGEHOG+prop+"}}"` markers.

    The file contains two different non-overlapping markers. Does not actually
    write the file, just returns text contents.
    """
    # âœ¨ tmp file with markers
    return '''
    # Some code
    def foo():
        pass # {{ðŸ¦” property1}}

    # More code
    class Bar:
        def baz(self):
            return 1 # {{ðŸ¦” property2}}
    '''
    # âœ¨

  async def build_workflow(
      self, scripted_messages: dict[str, list[Message]]
  ) -> CodeSpecsTestsSkeletonWorkflow:
    """Sets up a CodeSpecTestsSkeletonWorkflow with fake objects.

    Sets option skip_implicit_validation to True.
    """
    # âœ¨ build workflow
    fake_conversational_ai = FakeConversationalAI(scripted_messages)

    # `conversation`, `start_message`, and `commands_registry` in AgentLoopOptions
    # are placeholders here, as they will be overridden by `run_agent_loop`
    # when it creates the actual AgentLoop instance for a specific conversation.
    dummy_conversation = self.conversation_factory.New(
        "dummy_conversation_for_options", CommandRegistry())
    dummy_message = Message(
        role="user", content_sections=[ContentSection(content="dummy")])
    empty_command_registry = CommandRegistry()

    test_confirmation_manager = TestConfirmationManager()
    test_confirmation_state = TestConfirmationState(
        confirmation_manager=test_confirmation_manager)

    agent_loop_options = AgentLoopOptions(
        conversation=dummy_conversation,
        start_message=dummy_message,
        commands_registry=empty_command_registry,
        confirmation_state=test_confirmation_state,
        file_access_policy=TestFileAccessPolicy(),
        conversational_ai=fake_conversational_ai,
        confirm_regex=None,
        skip_implicit_validation=True,  # Set to True as per the requirement
        validation_manager=None,
    )

    agent_loop_factory = AgentLoopFactory()

    agent_workflow_options = AgentWorkflowOptions(
        agent_loop_options=agent_loop_options,
        agent_loop_factory=agent_loop_factory,
        conversation_factory=self.conversation_factory,
        selection_manager=TestSelectionManager(),
        principle_paths=None,
        input_paths=None,
        original_task_prompt_content=None,
        confirm_done='',
        do_review=False,
        review_first=False,
    )

    return CodeSpecsTestsSkeletonWorkflow(agent_workflow_options)
    # âœ¨

  def filter_content_sections(
      self, predicate: Callable[[ContentSection],
                                bool]) -> list[ContentSection]:
    """Returns any content sections in any messages that match a predicate.

    Extracts the messages from self.conversation_factory."""
    # âœ¨ filter content sections
    matching_sections: list[ContentSection] = []
    for conversation in self.conversation_factory.GetAll():
      for message in conversation.GetMessagesList():
        for section in message.GetContentSections():
          if predicate(section):
            matching_sections.append(section)
    return matching_sections
    # âœ¨

  async def test_get_initial_parameters_only_path_to_test_variable_argument(
      self) -> None:
    # âœ¨ The only done command argument given to `prepare_command_registry` is `path_to_test_variable`.
    async def test_get_initial_parameters_only_path_to_test_variable_argument(
        self) -> None:
      # 1. Prepare a valid file content with a HEDGEHOG marker using the helper.
      valid_file_content = self.get_valid_contents_with_hedgehog_markers()

      # 2. Create a message that simulates the user providing a valid path.
      #    The done_message_for_file helper writes the content to a temporary file
      #    and returns a Message with the path in path_to_test_variable.
      done_message = await self.done_message_for_file(valid_file_content)

      # 3. Script the FakeConversationalAI to use this done_message for the
      #    "initial_parameters" conversation.
      scripted_messages = {"initial_parameters": [done_message]}

      # 4. Build the workflow with the scripted messages.
      workflow = await self.build_workflow(scripted_messages)

      # 5. Run _get_initial_parameters. This will execute an agent loop,
      #    and the FakeConversationalAI will record the conversation.
      await workflow._get_initial_parameters()

      # 6. Access the FakeConversationalAI object from the workflow's options.
      fake_conversational_ai = workflow._options.agent_loop_options.conversational_ai
      self.assertIsInstance(fake_conversational_ai, FakeConversationalAI)

      # 7. Get the "initial_parameters" conversation from FakeConversationalAI.
      #    The conversation_name is used as the key.
      conversation_id = ConversationId("initial_parameters")
      self.assertIn(conversation_id, fake_conversational_ai.conversations)
      initial_parameters_conversation = fake_conversational_ai.conversations[
          conversation_id]

      # 8. Get the command registry from this conversation.
      command_registry = initial_parameters_conversation._command_registry
      self.assertIsInstance(command_registry, CommandRegistry)

      # 9. Find the DoneCommand in the command registry.
      done_command = None
      for command in command_registry.GetAllCommands():
        if isinstance(command, DoneCommand):
          done_command = command
          break

      self.assertIsNotNone(done_command, "DoneCommand not found in registry.")
      # Ensure mypy is happy by checking that done_command is not None before using it.
      if done_command is not None:
        # 10. Assert that the DoneCommand has only `path_to_test_variable` as its argument.
        done_syntax_arguments = done_command.Syntax().arguments
        self.assertEqual(len(done_syntax_arguments), 1)
        self.assertEqual(done_syntax_arguments[0].name, path_to_test_variable)

    # âœ¨

  async def test_initial_parameters_validator_fails_if_file_cant_be_read(
      self) -> None:
    # âœ¨ Validation fails if the file can't be read.
    # Helper to create an assistant message that issues a done command for a given path.
    def _create_done_command_assistant_message(path: pathlib.Path) -> Message:
      return Message(
          role="assistant",  # Role is 'assistant' as this is what the FakeConversationalAI expects as its scripted responses
          content_sections=[
              ContentSection(
                  content=f"Providing path: {path}",
                  command=CommandInput(
                      command_name="done",
                      args=VariableMap({
                          path_to_test_variable: VariableValueStr(str(path))
                      })),
                  summary=f"Done command for path: {path}")
          ])

    # 1. Create a path to a non-existent file.
    non_existent_path = pathlib.Path("/non/existent/path/to/test_file.py")

    # 2. Create a valid temporary file with HEDGEHOG markers.
    valid_file_content = self.get_valid_contents_with_hedgehog_markers()
    f = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.py')
    f.write(valid_file_content)
    f.close()
    valid_tmp_file_path = pathlib.Path(f.name)
    self.addCleanup(os.remove, valid_tmp_file_path)

    # 3. Script the FakeConversationalAI to first issue a done command with the non-existent path,
    #    then, after validation failure, issue a done command with the valid path.
    scripted_messages = {
        "initial_parameters": [
            _create_done_command_assistant_message(non_existent_path),
            _create_done_command_assistant_message(valid_tmp_file_path)
        ]
    }

    # 4. Build the workflow with the scripted messages.
    workflow = await self.build_workflow(scripted_messages)

    # 5. Run _get_initial_parameters. It should now complete successfully
    #    because the second 'done' message in scripted_messages provides a valid path.
    returned_path = await workflow._get_initial_parameters()

    # 6. Assert that the returned path is the valid one.
    self.assertEqual(returned_path, valid_tmp_file_path)

    # 7. Verify that the conversation history contains an error message
    #    indicating the validation failure for the first (non-existent) path,
    #    using self.filter_content_sections.
    expected_error_substring_part1 = "Argument path_to_test: File not found: Value:"
    expected_error_substring_part2 = "Warning done:"

    def predicate(section: ContentSection) -> bool:
      # The predicate will check for content sections from *any* message
      # that contains both parts of the expected error message and the non_existent_path.
      return (expected_error_substring_part1 in section.content and
              expected_error_substring_part2 in section.content and
              str(non_existent_path) in section.content)

    matching_error_sections = self.filter_content_sections(predicate)

    # Assert that at least one such error message was found. This indicates the validation failed and was reported.
    self.assertGreaterEqual(
        len(matching_error_sections), 1,
        f"Expected validation error message for path '{non_existent_path}' not found in conversation sections."
    )
    # âœ¨

  async def test_initial_parameters_validator_fails_if_no_hedgehog_markers(
      self) -> None:
    # âœ¨ Validation fails if the file doesn't contain any HEDGEHOG markers (per `code_specs.get_markers`).
    # 1. Create a temporary file that is readable but *does not* contain any HEDGEHOG markers.
    no_hedgehog_content = """
    # This is a test file without any HEDGEHOG markers.
    def some_function():
        return 1

    class SomeClass:
        pass
    """
    f_no_hedgehog = tempfile.NamedTemporaryFile(
        mode='w', delete=False, suffix='.py')
    f_no_hedgehog.write(no_hedgehog_content)
    f_no_hedgehog.close()
    no_hedgehog_path = pathlib.Path(f_no_hedgehog.name)
    self.addCleanup(os.remove, no_hedgehog_path)

    # Helper to create an assistant message that issues a done command for a given path.
    # This is similar to done_message_for_file but constructs the message directly
    # for a pre-existing path, rather than creating a new temp file.
    def _create_done_command_assistant_message(path: pathlib.Path) -> Message:
      return Message(
          role="assistant",
          content_sections=[
              ContentSection(
                  content=f"Providing path: {path}",
                  command=CommandInput(
                      command_name="done",
                      args=VariableMap({
                          path_to_test_variable: VariableValueStr(str(path))
                      })),
                  summary=f"Done command for path: {path}")
          ])

    # 2. Create a valid temporary file with HEDGEHOG markers to allow the workflow to eventually succeed.
    valid_file_content = self.get_valid_contents_with_hedgehog_markers()
    valid_done_message = await self.done_message_for_file(valid_file_content)
    # Extract the actual path from the valid_done_message, as done_message_for_file creates its own temp file.
    # Add a check for valid_done_message.GetContentSections()[0].command being not None
    command_input = valid_done_message.GetContentSections()[0].command
    if command_input and command_input.args:
      valid_path_from_message = pathlib.Path(
          str(command_input.args[path_to_test_variable]))
    else:
      # Handle the case where command_input or command_input.args is None if it can happen,
      # or raise an error if it should never happen in a valid test setup.
      # For now, let's assume it should always be present in a valid_done_message for this test.
      raise ValueError(
          "CommandInput or its arguments not found in valid_done_message.")

    # 3. Script the FakeConversationalAI: first, try to validate the file without HEDGEHOG markers (expected to fail),
    #    then provide the valid file (expected to succeed).
    scripted_messages = {
        "initial_parameters": [
            _create_done_command_assistant_message(no_hedgehog_path),
            valid_done_message,
        ]
    }

    # 4. Build the workflow with the scripted messages.
    workflow = await self.build_workflow(scripted_messages)

    # 5. Run _get_initial_parameters. It should now complete successfully
    #    because the second 'done' message in scripted_messages provides a valid path.
    returned_path = await workflow._get_initial_parameters()

    # 6. Assert that the returned path is the valid one.
    self.assertEqual(returned_path, valid_path_from_message)

    # 7. Verify that the conversation history contains an error message
    #    indicating the validation failure for the file without HEDGEHOG markers.
    expected_error_substring = (
        f"Error: File '{no_hedgehog_path}' does not contain any '{HEDGEHOG}' markers."
    )

    def predicate(section: ContentSection) -> bool:
      return expected_error_substring in section.content

    matching_error_sections = self.filter_content_sections(predicate)

    # Assert that at least one such error message was found.
    self.assertGreaterEqual(
        len(matching_error_sections), 1,
        f"Expected validation error message for path '{no_hedgehog_path}' "
        f"not found in conversation sections. Found: {matching_error_sections}")
    # âœ¨

  async def test_initial_parameters_validator_succeeds_with_readable_file_and_hedgehog_markers(
      self) -> None:
    # âœ¨ Validation succeeds if the file can be read and contains HEDGEHOG markers (per `code_specs.get_markers`).
    # Helper to create an assistant message that issues a done command for a given path.
    def _create_done_command_assistant_message(path: pathlib.Path) -> Message:
      return Message(
          role="assistant",
          content_sections=[
              ContentSection(
                  content=f"Providing path: {path}",
                  command=CommandInput(
                      command_name="done",
                      args=VariableMap({
                          path_to_test_variable: VariableValueStr(str(path))
                      })),
                  summary=f"Done command for path: {path}")
          ])

    # 1. Create a temporary file that is readable and contains HEDGEHOG markers.
    valid_file_content = self.get_valid_contents_with_hedgehog_markers()
    f = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.py')
    f.write(valid_file_content)
    f.close()
    valid_tmp_file_path = pathlib.Path(f.name)
    self.addCleanup(os.remove, valid_tmp_file_path)

    # 2. Script the FakeConversationalAI to issue a done command with this valid path.
    scripted_messages = {
        "initial_parameters": [
            _create_done_command_assistant_message(valid_tmp_file_path)
        ]
    }

    # 3. Build the workflow with the scripted messages.
    workflow = await self.build_workflow(scripted_messages)

    # 4. Run _get_initial_parameters. It should complete successfully.
    returned_path = await workflow._get_initial_parameters()

    # 5. Assert that the returned path is the valid one.
    self.assertEqual(returned_path, valid_tmp_file_path)

    # 6. Verify that no validation error messages are present in the conversation sections.
    #    We check for sections that start with "Error: " or contain "Warning done:"
    #    which indicate validation failures.
    def predicate_for_errors(section: ContentSection) -> bool:
      return section.content.startswith(
          "Error: ") or "Warning done:" in section.content

    matching_error_sections = self.filter_content_sections(predicate_for_errors)
    self.assertEqual(
        len(matching_error_sections), 0,
        f"No validation error messages were expected, but found: {matching_error_sections}"
    )
    # âœ¨

  async def test_initial_parameters_validator_succeeds_with_repeated_hedgehog_markers(
      self) -> None:
    # âœ¨ Validation succeeds if the file contains repeated (identical) HEDGEHOG markers (per `code_specs.get_markers`).
    # Helper to create an assistant message that issues a done command for a given path.
    def _create_done_command_assistant_message(path: pathlib.Path) -> Message:
      return Message(
          role="assistant",
          content_sections=[
              ContentSection(
                  content=f"Providing path: {path}",
                  command=CommandInput(
                      command_name="done",
                      args=VariableMap({
                          path_to_test_variable: VariableValueStr(str(path))
                      })),
                  summary=f"Done command for path: {path}")
          ])

    # 1. Create a temporary file with repeated identical HEDGEHOG markers.
    f = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.py')
    f.write('''# {{ðŸ¦” repeated_marker}}
        # {{ðŸ¦” repeated_marker}}
        ''')
    f.close()
    repeated_hedgehog_markers_path = pathlib.Path(f.name)
    self.addCleanup(os.remove, repeated_hedgehog_markers_path)

    # 2. Script the FakeConversationalAI to issue a done command with this valid path.
    scripted_messages = {
        "initial_parameters": [
            _create_done_command_assistant_message(
                repeated_hedgehog_markers_path)
        ]
    }

    # 3. Build the workflow with the scripted messages.
    workflow = await self.build_workflow(scripted_messages)

    # 4. Run _get_initial_parameters. It should complete successfully.
    returned_path = await workflow._get_initial_parameters()

    # 5. Assert that the returned path is the valid one.
    self.assertEqual(returned_path, repeated_hedgehog_markers_path)

    # 6. Verify that no validation error messages are present in the conversation sections.
    def predicate_for_errors(section: ContentSection) -> bool:
      return section.content.startswith(
          "Error: ") or "Warning done:" in section.content

    matching_error_sections = self.filter_content_sections(predicate_for_errors)
    self.assertEqual(
        len(matching_error_sections), 0,
        f"No validation error messages were expected, but found: {matching_error_sections}"
    )

  # âœ¨

  async def test_initial_parameters_validator_only_requires_path_to_test_variable(
      self) -> None:
    # âœ¨ Does not require any variables other than `path_to_test_variable`.
    # 1. Prepare valid file content with HEDGEHOG markers using the helper.
    valid_file_content = self.get_valid_contents_with_hedgehog_markers()

    # 2. Create a message that simulates the user providing a valid path.
    #    The done_message_for_file helper writes the content to a temporary file
    #    and returns a Message with the path in path_to_test_variable.
    done_message = await self.done_message_for_file(valid_file_content)

    # Extract the actual path from the done_message, as done_message_for_file creates its own temp file.
    command_input = done_message.GetContentSections()[0].command
    if command_input and command_input.args:
      valid_tmp_file_path = pathlib.Path(
          str(command_input.args[path_to_test_variable]))
    else:
      raise ValueError(
          "CommandInput or its arguments not found in done_message.")

    # 3. Script the FakeConversationalAI to issue a done command with only the valid path.
    scripted_messages = {"initial_parameters": [done_message]}

    # 4. Build the workflow with the scripted messages.
    workflow = await self.build_workflow(scripted_messages)

    # 5. Run _get_initial_parameters. It should complete successfully.
    returned_path = await workflow._get_initial_parameters()

    # 6. Assert that the returned path is the valid one.
    self.assertEqual(returned_path, valid_tmp_file_path)

    # 7. Verify that no validation error messages are present in the conversation sections.
    def predicate_for_errors(section: ContentSection) -> bool:
      return section.content.startswith(
          "Error: ") or "Warning done:" in section.content

    matching_error_sections = self.filter_content_sections(predicate_for_errors)
    self.assertEqual(
        len(matching_error_sections), 0,
        f"No validation error messages were expected, but found: {matching_error_sections}"
    )
    # âœ¨

  async def test_prepare_tests_skeleton_only_tests_skeleton_variable_argument(
      self) -> None:

    # âœ¨ The only done command argument given to `prepare_command_registry` in `_prepare_tests_skeleton` is `tests_skeleton_variable`.
    async def test_prepare_tests_skeleton_only_tests_skeleton_variable_argument(
        self) -> None:
      # 1. Prepare a valid file content with HEDGEHOG markers.
      valid_file_content = """
          # {{ðŸ¦” A test property}}
          def some_function():
              pass
          """

      # 2. Create a dummy path_to_test as _prepare_tests_skeleton expects it.
      #    The actual content of this file is not relevant for this specific test
      #    as we are only testing the done command arguments for _prepare_tests_skeleton.
      f = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.py')
      f.write(valid_file_content)
      f.close()
      tmp_input_path = pathlib.Path(f.name)
      self.addCleanup(os.remove, tmp_input_path)

      # 3. Create a message that simulates the user providing a valid tests skeleton.
      #    The content of the skeleton doesn't matter for this test,
      #    only that the done command is called with the correct variable name.
      done_command_args = VariableMap({
          tests_skeleton_variable:
              VariableValueStr("Some generated skeleton content.")
      })
      done_message = Message(
          role="assistant",  # Changed from "user" to "assistant"
          content_sections=[
              ContentSection(
                  content=f"Calling done command with {tests_skeleton_variable} = '...' ",
                  command=CommandInput(
                      command_name="done", args=done_command_args),
                  summary=f"Done command for tests skeleton")
          ])

      # 4. Script the FakeConversationalAI to use this done_message for the
      #    "prepare_tests_skeleton" conversation.
      scripted_messages = {"prepare_tests_skeleton": [done_message]}

      # 5. Build the workflow with the scripted messages.
      workflow = await self.build_workflow(scripted_messages)

      # 6. Run _prepare_tests_skeleton. This will execute an agent loop,
      #    and the FakeConversationalAI will record the conversation.
      await workflow._prepare_tests_skeleton(tmp_input_path)

      # 7. Access the FakeConversationalAI object from the workflow's options.
      fake_conversational_ai = workflow._options.agent_loop_options.conversational_ai
      self.assertIsInstance(fake_conversational_ai, FakeConversationalAI)

      # 8. Get the "prepare_tests_skeleton" conversation from FakeConversationalAI.
      conversation_id = ConversationId("prepare_tests_skeleton")
      self.assertIn(conversation_id, fake_conversational_ai.conversations)
      prepare_tests_skeleton_conversation = fake_conversational_ai.conversations[
          conversation_id]

      # 9. Get the command registry from this conversation.
      command_registry = prepare_tests_skeleton_conversation._command_registry
      self.assertIsInstance(command_registry, CommandRegistry)

      # 10. Find the DoneCommand in the command registry.
      done_command = None
      for command in command_registry.GetAllCommands():
        if isinstance(command, DoneCommand):
          done_command = command
          break

      self.assertIsNotNone(done_command, "DoneCommand not found in registry.")
      # Ensure mypy is happy by checking that done_command is not None before using it.
      if done_command is not None:
        # 11. Assert that the DoneCommand has only `tests_skeleton_variable` as its argument.
        done_syntax_arguments = done_command.Syntax().arguments
        self.assertEqual(len(done_syntax_arguments), 1)
        self.assertEqual(done_syntax_arguments[0].name, tests_skeleton_variable)

    # âœ¨

  async def test_prepare_tests_skeleton_writes_to_test_file_after_validation(
      self) -> None:
    # âœ¨ After validating that the skeleton contains all tests, writes them to a `tests_â€¦` file (e.g., when `input` is `src/foo.py`, writes `src/test_foo.py`).
    async def test_prepare_tests_skeleton_writes_to_test_file_after_validation(
        self) -> None:
      # 1. Test case: input file is 'src/foo.py' -> output file 'src/test_foo.py'
      input_file_content_py = """
          # Some code
          def func_to_test_py():
              pass # {{ðŸ¦” property for py file}}
          """
      with tempfile.NamedTemporaryFile(
          mode='w', delete=False, suffix='.py') as f:
        f.write(input_file_content_py)
        tmp_input_py_path = pathlib.Path(f.name)
      self.addCleanup(os.remove, tmp_input_py_path)

      expected_skeleton_content_py = (
          "import unittest\n"
          "\n"
          "class TestFuncToTestPy(unittest.IsolatedAsyncioTestCase):\n"
          "  async def test_func_to_test_py_property(self) -> None:\n"
          "    pass  # {{" + MUSHROOM + " property for py file}}\n"
          "\n"
          "if __name__ == '__main__':\n"
          "  unittest.main()\n")
      # 2. Test case: input file is 'src/foo.dm.py' -> output file 'src/test_foo.py'
      input_file_content_dm_py = """
          # Some code in a .dm.py file
          def func_to_test_dm_py():
              pass # {{ðŸ¦” property for dm py file}}
          """
      with tempfile.NamedTemporaryFile(
          mode='w', delete=False, suffix='.dm.py') as f:
        f.write(input_file_content_dm_py)
        tmp_input_dm_py_path = pathlib.Path(f.name)
      self.addCleanup(os.remove, tmp_input_dm_py_path)

      expected_skeleton_content_dm_py = (
          "import unittest\n"
          "\n"
          "class TestFuncToTestDmPy(unittest.IsolatedAsyncioTestCase):\n"
          "  async def test_func_to_test_py_property(self) -> None:\n"
          "    pass  # {{" + MUSHROOM + " property for dm py file}}\n"
          "\n"
          "if __name__ == '__main__':\n"
          "  unittest.main()\n")

      # Scripted messages for the first workflow run (.py input)
      scripted_messages_py = {
          "initial_parameters": [
              Message(
                  role="assistant",
                  content_sections=[
                      ContentSection(
                          content=f"Providing path: {tmp_input_py_path}",
                          command=CommandInput(
                              command_name="done",
                              args=VariableMap({
                                  path_to_test_variable:
                                      VariableValueStr(str(tmp_input_py_path))
                              })),
                          summary=f"Done command for path: {tmp_input_py_path}")
                  ])
          ],
          "prepare_tests_skeleton": [
              Message(
                  role="assistant",
                  content_sections=[
                      ContentSection(
                          content="Providing generated tests skeleton for .py file.",
                          command=CommandInput(
                              command_name="done",
                              args=VariableMap({
                                  tests_skeleton_variable:
                                      VariableValueStr(
                                          expected_skeleton_content_py)
                              })),
                          summary="Done command for tests skeleton for .py file"
                      )
                  ])
          ]
      }

      # Build and run the workflow for .py input
      workflow_py = await self.build_workflow(scripted_messages_py)
      await workflow_py.run()

      # Verify the output file for .py input
      output_py_file_name = f"test_{tmp_input_py_path.stem}{tmp_input_py_path.suffix}"
      expected_output_py_path = tmp_input_py_path.parent / output_py_file_name

      self.assertTrue(
          expected_output_py_path.exists(),
          f"Output file {expected_output_py_path} was not created for .py input."
      )

      async with aiofiles.open(expected_output_py_path, mode='r') as f:
        actual_output_content_py = await f.read()

      self.assertEqual(
          actual_output_content_py.strip(),
          expected_skeleton_content_py.strip(),
          "The content of the generated test file for .py input does not match the expected skeleton."
      )

      # Scripted messages for the second workflow run (.dm.py input)
      scripted_messages_dm_py = {
          "initial_parameters": [
              Message(
                  role="assistant",
                  content_sections=[
                      ContentSection(
                          content=f"Providing path: {tmp_input_dm_py_path}",
                          command=CommandInput(
                              command_name="done",
                              args=VariableMap({
                                  path_to_test_variable:
                                      VariableValueStr(
                                          str(tmp_input_dm_py_path))
                              })),
                          summary=f"Done command for path: {tmp_input_dm_py_path}"
                      )
                  ])
          ],
          "prepare_tests_skeleton": [
              Message(
                  role="assistant",
                  content_sections=[
                      ContentSection(
                          content="Providing generated tests skeleton for .dm.py file.",
                          command=CommandInput(
                              command_name="done",
                              args=VariableMap({
                                  tests_skeleton_variable:
                                      VariableValueStr(
                                          expected_skeleton_content_dm_py)
                              })),
                          summary="Done command for tests skeleton for .dm.py file"
                      )
                  ])
          ]
      }

      # Build and run the workflow for .dm.py input
      workflow_dm_py = await self.build_workflow(scripted_messages_dm_py)
      await workflow_dm_py.run()

      # Verify the output file for .dm.py input
      output_dm_py_file_name = f"test_{tmp_input_dm_py_path.stem.replace('.dm', '')}{tmp_input_dm_py_path.suffix}"
      expected_output_dm_py_path = tmp_input_dm_py_path.parent / output_dm_py_file_name

      self.assertTrue(
          expected_output_dm_py_path.exists(),
          f"Output file {expected_output_dm_py_path} was not created for .dm.py input."
      )

      async with aiofiles.open(expected_output_dm_py_path, mode='r') as f:
        actual_output_content_dm_py = await f.read()

      self.assertEqual(
          actual_output_content_dm_py.strip(),
          expected_skeleton_content_dm_py.strip(),
          "The content of the generated test file for .dm.py input does not match the expected skeleton."
      )

    # âœ¨

  async def test_prepare_tests_skeleton_input_passed_as_relevant_file(
      self) -> None:
    # âœ¨ `input` is passed as a relevant file to `prepare_initial_message`.
    # 1. Create a temporary input file.
    input_file_content = """
    # Some code.
    def foo():
        pass # {{ðŸ¦” property for testing relevant files}}
    """
    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.py') as f:
      f.write(input_file_content)
      tmp_input_path = pathlib.Path(f.name)
    self.addCleanup(os.remove, tmp_input_path)

    # 2. Script FakeConversationalAI.
    #    First, for _get_initial_parameters to provide the input file.
    #    Second, for _prepare_tests_skeleton to provide a dummy skeleton (to allow it to complete).
    scripted_messages = {
        "initial_parameters": [
            Message(
                role="assistant",
                content_sections=[
                    ContentSection(
                        content=f"Providing path: {tmp_input_path}",
                        command=CommandInput(
                            command_name="done",
                            args=VariableMap({
                                path_to_test_variable:
                                    VariableValueStr(str(tmp_input_path))
                            })),
                        summary=f"Done command for path: {tmp_input_path}")
                ])
        ],
        "prepare_tests_skeleton": [
            Message(
                role="assistant",
                content_sections=[
                    ContentSection(
                        content="Providing generated tests skeleton.",
                        command=CommandInput(
                            command_name="done",
                            args=VariableMap({
                                tests_skeleton_variable:
                                    VariableValueStr(
                                        "def test_dummy(): pass # {{" +
                                        MUSHROOM + " dummy}}")
                            })),
                        summary="Done command for tests skeleton")
                ])
        ]
    }

    # 3. Build the workflow.
    workflow = await self.build_workflow(scripted_messages)

    # 4. Run the workflow. This will execute both _get_initial_parameters and _prepare_tests_skeleton.
    await workflow.run()

    # 5. Access the FakeConversationalAI object to get the conversation ID.
    fake_conversational_ai = workflow._options.agent_loop_options.conversational_ai
    self.assertIsInstance(fake_conversational_ai, FakeConversationalAI)

    # Use getattr as a workaround for mypy type inference to get the 'started_conversations' map.
    started_conversations_map = getattr(fake_conversational_ai,
                                        'started_conversations')

    # Get the integer conversation ID for "prepare_tests_skeleton" conversation.
    prepare_tests_skeleton_conv_id = started_conversations_map.get(
        "prepare_tests_skeleton")
    self.assertIsNotNone(
        prepare_tests_skeleton_conv_id,
        "Conversation ID for 'prepare_tests_skeleton' not found.")

    # 6. Iterate through all conversations in the factory to find the one with the matching ID.
    found_conversation: Conversation | None = None
    for conv in self.conversation_factory.GetAll():
      if conv.GetId() == prepare_tests_skeleton_conv_id:
        found_conversation = conv
        break

    self.assertIsNotNone(
        found_conversation,
        "Conversation with ID 'prepare_tests_skeleton_conv_id' not found in factory."
    )

    # Explicitly check for None to satisfy mypy's type narrowing.
    if found_conversation is not None:
      # 7. Get the list of messages in the found conversation.
      messages_list = found_conversation.GetMessagesList()

      # The initial message from the agent should be the first message in the conversation.
      # This is the 'start_message' that was prepared by prepare_initial_message.
      initial_message_from_agent: Message | None = None
      if messages_list:
        initial_message_from_agent = messages_list[0]

      self.assertIsNotNone(
          initial_message_from_agent,
          "Initial message from agent for 'prepare_tests_skeleton' conversation not found."
      )

      # Now, initial_message_from_agent is guaranteed to be a Message instance.
      # Use another 'if not None' check to satisfy mypy for the GetContentSections() call.
      if initial_message_from_agent is not None:
        # Check if any ContentSection contains the relevant file information.
        relevant_file_found = False
        expected_content_start = f"File '{tmp_input_path}' follows:"

        for section in initial_message_from_agent.GetContentSections():
          if section.content.startswith(expected_content_start):
            relevant_file_found = True
            break

        self.assertTrue(
            relevant_file_found,
            "The 'input' file content was not found as a relevant file section in the initial message for _prepare_tests_skeleton."
        )
      else:
        self.fail(
            "Logical error: initial_message_from_agent should not be None after assertion."
        )
    else:
      # This else block should theoretically not be reached because of assertIsNotNone above.
      self.fail(
          "Logical error: found_conversation should not be None after assertion."
      )
    # âœ¨

  async def test_tests_skeleton_validator_fails_if_markers_rejected_by_get_markers(
      self) -> None:
    # âœ¨ Fails if the set of markers given in tests_skeleton_variable is rejected by `code_spec.get_markers(MUSHROOM, output)`.
    # Helper to create an assistant message that issues a done command for a given skeleton.
    def _create_done_command_assistant_message(
        skeleton_content: str) -> Message:
      return Message(
          role="assistant",
          content_sections=[
              ContentSection(
                  content="Providing generated tests skeleton.",
                  command=CommandInput(
                      command_name="done",
                      args=VariableMap({
                          tests_skeleton_variable:
                              VariableValueStr(skeleton_content)
                      })),
                  summary="Done command for tests skeleton")
          ])

    # 1. Create a temporary input file with HEDGEHOG markers asynchronously.
    valid_hedgehog_content = self.get_valid_contents_with_hedgehog_markers()
    tmp_dir = tempfile.mkdtemp()
    tmp_input_file_path = pathlib.Path(tmp_dir) / "input_with_hedgehog.py"
    self.addCleanup(lambda: shutil.rmtree(tmp_dir))

    async with aiofiles.open(tmp_input_file_path, mode='w') as f:
      await f.write(valid_hedgehog_content)
    # This file has 2 HEDGEHOG markers.

    # 2. Define an invalid skeleton content that causes MarkersOverlapError.
    #    This skeleton has two MUSHROOM markers on the same line.
    invalid_skeleton_content = (
        "import unittest\n"
        "\n"
        "class MyTests(unittest.IsolatedAsyncioTestCase):\n"
        "  async def test_overlapping_markers_1(self) -> None:\n"
        "    pass  # {{" + MUSHROOM + " marker_overlap_1}}{{" + MUSHROOM +
        " marker_overlap_2}}\n"
        "  async def test_overlapping_markers_2(self) -> None:\n"
        "    pass  # {{" + MUSHROOM + " another_valid_marker}}\n"
        "\n"
        "if __name__ == '__main__':\n"
        "  unittest.main()\n")

    # 3. Define a valid skeleton content that passes all validations.
    #    It should have the same number of MUSHROOM markers (2) as HEDGEHOG markers in the input.
    valid_skeleton_content = (
        "import unittest\n"
        "\n"
        "class MyTests(unittest.IsolatedAsyncioTestCase):\n"
        "  async def test_valid_marker_1(self) -> None:\n"
        "    pass  # {{" + MUSHROOM + " valid_marker_1}}\n"
        "  async def test_valid_marker_2(self) -> None:\n"
        "    pass  # {{" + MUSHROOM + " valid_marker_2}}\n"
        "\n"
        "if __name__ == '__main__':\n"
        "  unittest.main()\n")

    # 4. Script FakeConversationalAI.
    scripted_messages = {
        "initial_parameters": [
            Message(
                role="assistant",
                content_sections=[
                    ContentSection(
                        content=f"Providing path: {tmp_input_file_path}",
                        command=CommandInput(
                            command_name="done",
                            args=VariableMap({
                                path_to_test_variable:
                                    VariableValueStr(str(tmp_input_file_path))
                            })),
                        summary=f"Done command for path: {tmp_input_file_path}")
                ])
        ],
        "prepare_tests_skeleton": [
            # First attempt: invalid skeleton
            _create_done_command_assistant_message(invalid_skeleton_content),
            # Second attempt: valid skeleton to allow the workflow to complete
            _create_done_command_assistant_message(valid_skeleton_content)
        ]
    }

    # 5. Build and run the workflow.
    workflow = await self.build_workflow(scripted_messages)
    await workflow.run()

    # 6. Verify that an error message related to MarkersOverlapError is present.
    expected_error_substring_part1 = "Generated skeleton contains overlapping 'ðŸ„' markers:"
    expected_error_substring_part2 = "Error: "

    def predicate_for_errors(section: ContentSection) -> bool:
      return section.content.startswith(
          expected_error_substring_part2
      ) and expected_error_substring_part1 in section.content

    matching_error_sections = self.filter_content_sections(predicate_for_errors)

    self.assertGreaterEqual(
        len(matching_error_sections), 1,
        f"Expected validation error message for overlapping MUSHROOM markers not found in conversation sections. Found: {matching_error_sections}"
    )

    # 7. Optionally, verify that the output file created is from the valid skeleton.
    #    This implies that the workflow recovered from the validation error and proceeded.
    output_file_name = f"test_{tmp_input_file_path.stem}{tmp_input_file_path.suffix}"
    expected_output_path = tmp_input_file_path.parent / output_file_name

    self.assertTrue(
        expected_output_path.exists(),
        f"Output file {expected_output_path} was not created after valid skeleton was provided."
    )

    async with aiofiles.open(expected_output_path, mode='r') as f:
      actual_output_content = await f.read()

    self.assertEqual(
        actual_output_content.strip(), valid_skeleton_content.strip(),
        "The content of the generated test file does not match the expected valid skeleton."
    )
    # âœ¨

  async def test_tests_skeleton_validator_fails_if_identical_mushroom_marker_repeated(
      self) -> None:
    # âœ¨ Fails if an identical marker (in the value of `tests_skeleton_variable`) is repeated (more than one location, per `code_spec.get_markers(MUSHROOM, output)`).
    # Helper to create an assistant message that issues a done command for a given skeleton.
    def _create_done_command_assistant_message(
        skeleton_content: str) -> Message:
      return Message(
          role="assistant",
          content_sections=[
              ContentSection(
                  content="Providing generated tests skeleton.",
                  command=CommandInput(
                      command_name="done",
                      args=VariableMap({
                          tests_skeleton_variable:
                              VariableValueStr(skeleton_content)
                      })),
                  summary="Done command for tests skeleton")
          ])

    # 1. Prepare a valid file content with HEDGEHOG markers using the helper.
    valid_file_content = self.get_valid_contents_with_hedgehog_markers()

    # Create a message that simulates the user providing a valid path.
    # The done_message_for_file helper writes the content to a temporary file
    # and returns a Message with the path in path_to_test_variable.
    done_message_for_initial_params = await self.done_message_for_file(
        valid_file_content)

    # Extract the actual path from the done_message, as done_message_for_file creates its own temp file.
    command_input = done_message_for_initial_params.GetContentSections(
    )[0].command
    if command_input and command_input.args:
      tmp_input_file_path = pathlib.Path(
          str(command_input.args[path_to_test_variable]))
    else:
      raise ValueError(
          "CommandInput or its arguments not found in done_message_for_initial_params."
      )
    # This file has 2 HEDGEHOG markers.

    # 2. Define an invalid skeleton content that causes a repeated MUSHROOM marker.
    #    This skeleton has the same MUSHROOM marker name used twice.
    invalid_skeleton_content = (
        "import unittest\n"
        "\n"
        "class MyTests(unittest.IsolatedAsyncioTestCase):\n"
        "  async def test_repeated_marker_1(self) -> None:\n"
        "    pass  # {{" + MUSHROOM + " repeated_marker_name}}\n"
        "  async def test_repeated_marker_2(self) -> None:\n"
        "    pass  # {{" + MUSHROOM + " repeated_marker_name}}\n"
        "\n"
        "if __name__ == '__main__':\n"
        "  unittest.main()\n")

    # 3. Define a valid skeleton content that passes all validations.
    #    It should have the same number of MUSHROOM markers (2) as HEDGEHOG markers in the input,
    #    and no repeated marker names.
    valid_skeleton_content = (
        "import unittest\n"
        "\n"
        "class MyTests(unittest.IsolatedAsyncioTestCase):\n"
        "  async def test_unique_marker_1(self) -> None:\n"
        "    pass  # {{" + MUSHROOM + " unique_marker_name_1}}\n"
        "  async def test_unique_marker_2(self) -> None:\n"
        "    pass  # {{" + MUSHROOM + " unique_marker_name_2}}\n"
        "\n"
        "if __name__ == '__main__':\n"
        "  unittest.main()\n")

    # 4. Script FakeConversationalAI.
    scripted_messages = {
        "initial_parameters": [done_message_for_initial_params],
        "prepare_tests_skeleton": [
            # First attempt: invalid skeleton with repeated MUSHROOM marker
            _create_done_command_assistant_message(invalid_skeleton_content),
            # Second attempt: valid skeleton to allow the workflow to complete
            _create_done_command_assistant_message(valid_skeleton_content)
        ]
    }

    # 5. Build and run the workflow.
    workflow = await self.build_workflow(scripted_messages)
    await workflow.run()

    # 6. Verify that an error message related to repeated MUSHROOM markers is present.
    expected_error_prefix = "Error: "
    expected_error_substring_part1 = f"MUSHROOM marker content '{{{MUSHROOM} MarkerName(char='{MUSHROOM}', name='repeated_marker_name')}}' is repeated 2 times in the skeleton."

    def predicate_for_errors(section: ContentSection) -> bool:
      return (section.content.startswith(expected_error_prefix) and
              expected_error_substring_part1 in section.content)

    matching_error_sections = self.filter_content_sections(predicate_for_errors)

    self.assertGreaterEqual(
        len(matching_error_sections), 1,
        f"Expected validation error message for repeated MUSHROOM marker not found in conversation sections. Found: {matching_error_sections}"
    )

    # 7. Verify that the output file created is from the valid skeleton.
    #    This implies that the workflow recovered from the validation error and proceeded.
    output_file_name = f"test_{tmp_input_file_path.stem}{tmp_input_file_path.suffix}"
    expected_output_path = tmp_input_file_path.parent / output_file_name

    self.assertTrue(
        expected_output_path.exists(),
        f"Output file {expected_output_path} was not created after valid skeleton was provided."
    )

    async with aiofiles.open(expected_output_path, mode='r') as f:
      actual_output_content = await f.read()

    self.assertEqual(
        actual_output_content.strip(), valid_skeleton_content.strip(),
        "The content of the generated test file does not match the expected valid skeleton."
    )
    # âœ¨

  async def test_tests_skeleton_validator_fails_if_marker_count_mismatch(
      self) -> None:
    # âœ¨ Fails if the number of HEDGEHOG markers in the input isn't exactly the same as the number of MUSHROOM markers in the output.
    # Helper to create an assistant message that issues a done command for a given skeleton.
    def _create_done_command_assistant_message(
        skeleton_content: str) -> Message:
      return Message(
          role="assistant",
          content_sections=[
              ContentSection(
                  content="Providing generated tests skeleton.",
                  command=CommandInput(
                      command_name="done",
                      args=VariableMap({
                          tests_skeleton_variable:
                              VariableValueStr(skeleton_content)
                      })),
                  summary="Done command for tests skeleton")
          ])

    # 1. Create a temporary input file with a specific number of HEDGEHOG markers (e.g., 2).
    # Re-use the helper that returns content, then write to a temp file.
    tmp_input_file_content = self.get_valid_contents_with_hedgehog_markers()
    # Use a temporary directory for the input file to control its lifecycle
    with tempfile.TemporaryDirectory() as tmpdir:
      tmp_input_file_path = pathlib.Path(tmpdir) / "input_for_test.py"
      async with aiofiles.open(tmp_input_file_path, mode='w') as f:
        await f.write(tmp_input_file_content)

      # 2. Define an invalid skeleton content with a different number of MUSHROOM markers (e.g., 1).
      invalid_skeleton_content = (
          '''import unittest
    '''
          '''
    '''
          '''class MyTests(unittest.IsolatedAsyncioTestCase):
    '''
          '''  async def test_single_mushroom_marker(self) -> None:
    '''
          '''    pass  # {{ðŸ„ single_mushroom}}
    '''
          '''
    '''
          '''if __name__ == '__main__':
    '''
          '''  unittest.main()
    ''')

      # 3. Define a valid skeleton content with the correct number of MUSHROOM markers (e.g., 2).
      valid_skeleton_content = (
          '''import unittest
    '''
          '''
    '''
          '''class MyTests(unittest.IsolatedAsyncioTestCase):
    '''
          '''  async def test_first_mushroom(self) -> None:
    '''
          '''    pass  # {{ðŸ„ property1}}
    '''
          '''  async def test_second_mushroom(self) -> None:
    '''
          '''    pass  # {{ðŸ„ property2}}
    '''
          '''
    '''
          '''if __name__ == '__main__':
    '''
          '''  unittest.main()
    ''')

      # 4. Script FakeConversationalAI.
      scripted_messages = {
          "initial_parameters": [
              # Provide the temporary input file for the first workflow run.
              Message(
                  role="assistant",
                  content_sections=[
                      ContentSection(
                          content=f"Providing path: {tmp_input_file_path}",
                          command=CommandInput(
                              command_name="done",
                              args=VariableMap({
                                  path_to_test_variable:
                                      VariableValueStr(
                                          str(tmp_input_file_path))
                              })),
                          summary=f"Done command for path: {tmp_input_file_path}"
                      )
                  ])
          ],
          "prepare_tests_skeleton": [
              # First attempt: invalid skeleton with marker count mismatch
              _create_done_command_assistant_message(invalid_skeleton_content),
              # Second attempt: valid skeleton to allow the workflow to complete
              _create_done_command_assistant_message(valid_skeleton_content)
          ]
      }

      # 5. Build and run the workflow.
      workflow = await self.build_workflow(scripted_messages)
      await workflow.run()

      # 6. Verify that an error message related to marker count mismatch is present.
      expected_error_prefix = "Error: "
      # Assuming 2 HEDGEHOG markers in input (from get_valid_contents_with_hedgehog_markers()),
      # and 1 MUSHROOM marker in invalid_skeleton_content.
      expected_error_substring_part1 = "Mismatch in marker counts: Input has 2 'ðŸ¦”' markers, but skeleton has 1 'ðŸ„' markers. Counts must be equal."

      def predicate_for_errors(section: ContentSection) -> bool:
        return (section.content.startswith(expected_error_prefix) and
                expected_error_substring_part1 in section.content)

      matching_error_sections = self.filter_content_sections(
          predicate_for_errors)

      self.assertGreaterEqual(
          len(matching_error_sections), 1,
          f"Expected validation error message for marker count mismatch not found in conversation sections. Found: {matching_error_sections}"
      )

      # 7. Verify that the output file created is from the valid skeleton.
      output_file_name = f"test_{tmp_input_file_path.stem}{tmp_input_file_path.suffix}"
      expected_output_path = tmp_input_file_path.parent / output_file_name

      self.assertTrue(
          expected_output_path.exists(),
          f"Output file {expected_output_path} was not created after valid skeleton was provided."
      )

      async with aiofiles.open(expected_output_path, mode='r') as f:
        actual_output_content = await f.read()

      self.assertEqual(
          actual_output_content.strip(), valid_skeleton_content.strip(),
          "The content of the generated test file does not match the expected valid skeleton."
      )
    # âœ¨

  async def test_tests_skeleton_validator_only_requires_tests_skeleton_variable(
      self) -> None:
    # âœ¨ Does not require any variables beyond `tests_skeleton_variable`.
    # Helper to create an assistant message that issues a done command for a given skeleton.
    def _create_done_command_assistant_message(
        skeleton_content: str) -> Message:
      return Message(
          role="assistant",
          content_sections=[
              ContentSection(
                  content="Providing generated tests skeleton.",
                  command=CommandInput(
                      command_name="done",
                      args=VariableMap({
                          tests_skeleton_variable:
                              VariableValueStr(skeleton_content)
                      })),
                  summary="Done command for tests skeleton")
          ])

    # 1. Create a temporary input file with HEDGEHOG markers asynchronously.
    valid_hedgehog_content = self.get_valid_contents_with_hedgehog_markers()
    tmp_dir = tempfile.mkdtemp()
    tmp_input_file_path = pathlib.Path(tmp_dir) / "input_with_hedgehog.py"
    self.addCleanup(lambda: shutil.rmtree(tmp_dir))

    async with aiofiles.open(tmp_input_file_path, mode='w') as f:
      await f.write(valid_hedgehog_content)
    # This file has 2 HEDGEHOG markers.

    # 2. Define a valid skeleton content. It should have 2 MUSHROOM markers
    #    to match the number of HEDGEHOG markers in the input.
    valid_skeleton_content = (
        "import unittest\n"
        "\n"
        "class MyTests(unittest.IsolatedAsyncioTestCase):\n"
        "  async def test_first_property(self) -> None:\n"
        "    pass  # {{" + MUSHROOM + " property1}}\n"
        "  async def test_second_property(self) -> None:\n"
        "    pass  # {{" + MUSHROOM + " property2}}\n"
        "\n"
        "if __name__ == '__main__':\n"
        "  unittest.main()\n")

    # 3. Script FakeConversationalAI.
    scripted_messages = {
        "initial_parameters": [
            Message(
                role="assistant",
                content_sections=[
                    ContentSection(
                        content=f"Providing path: {tmp_input_file_path}",
                        command=CommandInput(
                            command_name="done",
                            args=VariableMap({
                                path_to_test_variable:
                                    VariableValueStr(str(tmp_input_file_path))
                            })),
                        summary=f"Done command for path: {tmp_input_file_path}")
                ])
        ],
        "prepare_tests_skeleton": [
            # Provide the valid skeleton. This should pass as no other variables are required.
            _create_done_command_assistant_message(valid_skeleton_content)
        ]
    }

    # 4. Build and run the workflow.
    workflow = await self.build_workflow(scripted_messages)
    await workflow.run()

    # 5. Verify that no validation error messages are present in the conversation sections.
    #    This confirms that only `tests_skeleton_variable` was needed and the validation passed.
    def predicate_for_errors(section: ContentSection) -> bool:
      return section.content.startswith(
          "Error: ") or "Warning done:" in section.content

    matching_error_sections = self.filter_content_sections(predicate_for_errors)
    self.assertEqual(
        len(matching_error_sections), 0,
        f"No validation error messages were expected, but found: {matching_error_sections}"
    )

    # 6. Optionally, verify that the output file was created with the valid skeleton content.
    output_file_name = f"test_{tmp_input_file_path.stem}{tmp_input_file_path.suffix}"
    expected_output_path = tmp_input_file_path.parent / output_file_name

    self.assertTrue(expected_output_path.exists(),
                    f"Output file {expected_output_path} was not created.")

    async with aiofiles.open(expected_output_path, mode='r') as f:
      actual_output_content = await f.read()

    self.assertEqual(
        actual_output_content.strip(), valid_skeleton_content.strip(),
        "The content of the generated test file does not match the expected valid skeleton."
    )
    # âœ¨


if __name__ == '__main__':
  unittest.main()
